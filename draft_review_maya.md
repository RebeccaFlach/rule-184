# Draft Review of “Eliminating Disconnected Communities: A Comparative Study on Louvain and Leiden”
#### Reviewed by Maya Cranor

### Question:  What is your understanding of the experiment the team is replicating?  What question does it answer?  How clear is the team's explanation?
The team gives a great explanation on their motivation to do this experiment. However, I wish they had included a brief overview about both algorithms, so that the reader can have a better base understanding of the algorithms. I also think pictures would be nice to explain the models.
### Methodology: Do you understand the methodology?  Does it make sense for the question?  Are there limitations you see that the team did not address?
The draft report doesn’t really dive into the methodology besides stating that they ran the models on one data set and describing the data set. The lack of explanation of how the models work, makes it hard to understand their methodology. I did like the explanation of the data set, but it does leave me wondering if the size of the data set will have an effect on the result (edit: I see this mentioned later in the report, might be nice to explain your choices of data set earlier).
### Results: Do you understand what the results are (not yet considering their interpretation)?  If they are presented graphically, are the visualizations effective?  Do all figures have labels on the axes and captions?
It made me sad that the figures weren’t in the report yet. The figure in the code should be labeled. If the numbers are important to see, the dark blue and purple makes them very hard to read.
### Interpretation: Does the draft report interpret the results as an answer to the motivating question?  Does the argument hold water?
Interesting results, good job explaining the graph. Reasoning for the results are described well. Good luck exploring these issues!
### Replication: Are the results in the report consistent with the results from the original paper?  If so, how did the authors demonstrate that consistency?  Is it quantitative or qualitative?
The results are not consistent, which is highlighted by the question the team is exploring. 
### Extension: Does the report explain an extension to the original experiment clearly?  Can it answer an interesting question that the original experiment did not answer?
Extension seems interesting and it continues to prove which algorithm is better, however I’m not sure how much it answers a question that the original experiment did not cover. I’m also wondering how hard it will be to find more data.
### Progress: Is the team roughly where they should be at this point, with a replication that is substantially complete and an extension that is clearly defined and either complete or nearly so?
It’s hard to tell how much debugging they did need to do on their experiment. If they can get the experiment to show expected results or discover that the results are wrong in the next day or two, they are probably on track.
### Presentation: Is the report written in clear, concise, correct language?  Is it consistent with the audience and goals of the report?  Does it violate any of the recommendations in my style guide
Links to an external site.?
Looks good to me!
### Mechanics: Is the report in the right directory with the right file name?  Is it formatted professionally in Markdown?  Does it include a meaningful title and the full names of the authors?  Is the bibliography in an acceptable style?
Seems good to me!

